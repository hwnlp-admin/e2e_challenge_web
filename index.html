<!DOCTYPE html>
<html>
<title>E2E NLG Challenge</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/lib/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata">
<style>
body, html {
    height: 100%;
    font-family: "Lato", sans-serif;
}
.bgimg {
    background-position: center;
    background-size: cover;
    background-image: url("backgr.jpg");
    background-color: #888;
    min-height: 40%;
}
.menu {
    display: none;
}
.atcharhere:before {
    content: '@';
}
.w3-col.s4.m3.l2{
  width: 13%;
}
.late {
    font-style: italic;
}
.mid-align {
    vertical-align: middle !important; 
}
.notfirst {
    padding-left: 8px !important;
}

#main-title {
    font-size: 90px;
    padding-top: 1em;
    padding-bottom: 30px;
    line-height: 1;
}
dt {
    font-weight: bold;
}
pre {
    font-family: "Inconsolata", monospace;
}
h1, h2, h3, h4, h5, h6, h7 {
    font-family: "Open Sans", sans-serif;
}
h4 {
    font-weight: bold;
}
</style>

<!-- Google Analytics + autotrack -->
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-113061228-1', 'auto');

ga('require', 'eventTracker');
ga('require', 'outboundLinkTracker');

ga('send', 'pageview');
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<script async src="js/autotrack.js"></script>

<!-- table sorting script -->
<script type="text/javascript" src="js/sorttable.js"></script>

<!-- 

    body 

-->
<body>

<!-- Links (sit on top) -->
<div class="w3-top">
  <div class="w3-row w3-padding w3-black">
    <div class="w3-col s4 m3 l1">
      <a href="#" class="w3-button w3-block w3-black">HOME</a>
    </div>
    <div class="w3-col s4 m3 l1">
      <a href="#motiv" class="w3-button w3-block w3-black">MOTIVATION</a>
    </div>
    <div class="w3-col s4 m3 l2">
      <a href="#task" class="w3-button w3-block w3-black">TASK</a>
    </div>
    <!--<div class="w3-col s4 m3 l2">
      <a href="#news" class="w3-button w3-block w3-black">NEWS</a>
    </div>-->
    <div class="w3-col s4 m3 l2">
      <a href="#data" class="w3-button w3-block w3-black">DATA</a>
    </div>
    <div class="w3-col s4 m3 l2">
      <a href="#baseline" class="w3-button w3-block w3-black">BASELINE</a>
    </div>
    <div class="w3-col s4 m3 l1">
      <a href="#dates" class="w3-button w3-block w3-black">DATES</a>
    </div>
    <!--<div class="w3-col s4 m3 l1">
      <a href="#submission" class="w3-button w3-block w3-black">SUBMISSION</a>
    </div>-->
    <div class="w3-col s4 m3 l2">
      <a href="#results" class="w3-button w3-block w3-black">RESULTS</a>
    </div>
    <div class="w3-col s4 m3 l2">
      <a href="#papers" class="w3-button w3-block w3-black">PROCEEDINGS</a>
    </div>
    <div class="w3-col s4 m3 l1">
      <a href="#contacts" class="w3-button w3-block w3-black">CONTACTS</a>
    </div>
  </div>
</div>

<!-- Header with image -->
<header class="bgimg w3-display-container w3-grayscale-min" id="home">
  <div class="w3-display-bottomleft w3-center w3-padding-xlarge w3-hide-small">
    <span class="w3-tag">Data Release: 27 March 2017</span>
  </div>
  <div class="w3-center">
    <h1 class="w3-text-white" id="main-title">E2E<br>NLG Challenge</h1>
  </div>
  <div class="w3-display-bottomright w3-center w3-padding-xlarge">
    <span class="w3-text-white">Entry Submission Deadline: 31 October 2017</span>
  </div>
</header>

<!-- Add a background color and large text to the whole page -->
<div class="w3-sand w3-grayscale w3-large">



<!--
***
Start main content
***
-->

<!-- Motivation -->

<div class="w3-container" id="motiv">
  <div class="w3-content" style="max-width:700px">

<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
Motivation
</span></h2>

<p>
Natural language generation plays a critical role for Conversational Agents as it has a significant impact on a user’s impression of the system. This shared task focuses on recent end-to-end (E2E), data-driven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data, e.g. (<a href="http://aclweb.org/anthology/D15-1199">Wen et al., 2015</a>; <a href="http://www.aclweb.org/anthology/N16-1086">Mei et al., 2016</a>; <a href="http://www.aclweb.org/anthology/P16-2008">Dusek and Jurcicek, 2016</a>; <a href="https://aclweb.org/anthology/C16-1105">Lampouras and Vlachos, 2016</a>) etc.
</p>
<p>
So far, E2E NLG approaches were limited to small, de-lexicalised data sets, e.g. BAGEL, SF Hotels/ Restaurants, or RoboCup. In this shared challenge, we will provide a new crowd-sourced data set of 50k instances in the restaurant domain, as described in (<a href="http://aclweb.org/anthology/W16-6644">Novikova, Lemon and Rieser, 2016</a>). Each instance consist of a dialogue act-based meaning representation (MR) and up to 5 references in natural language. In contrast to previously used data, our data set includes additional challenges, such as open vocabulary, complex syntactic structures and diverse discourse phenomena. For example:
</p>
<div class="w3-panel w3-leftbar w3-light-grey">
<p><strong>MR:</strong></p>
<pre>
name[The Eagle],
eatType[coffee shop],
food[French],
priceRange[moderate],
customerRating[3/5],
area[riverside],
kidsFriendly[yes],
near[Burger King]
</pre>

<p><strong>NL:</strong></p>
<p><i>
“The three star coffee shop, The Eagle, gives families a mid-priced dining experience featuring a variety of wines and cheeses. Find The Eagle near Burger King.”
</i></p>
</div>

<p>
The full data set can now be downloaded <a href="https://github.com/tuetschek/e2e-dataset/releases/download/v1.0.0/e2e-dataset.zip">here</a>. A detailed description of the data can be found in our <a href="https://arxiv.org/abs/1706.09254">SIGDIAL 2017 paper</a>. A brief summary of the E2E NLG Challenge results is now available in our <a href="https://arxiv.org/abs/1810.01170">INLG 2018 paper</a>.
</p>
<p>
This challenge follows on from previous successful shared tasks on generation, e.g. <a href="http://alt.qcri.org/semeval2017/task9/">SemEval’17 task 9</a> on text generation from AMR, and <a href="http://www.nltg.brighton.ac.uk/research/genchal11/">Generation Challenges 2008-11</a>.  However, this is the first NLG task to concentrate on (1) generation from dialogue acts, (2) using semantically un-aligned data.
</p>
  </div>
</div>


<!-- The task -->

<div class="w3-container" id="task">
  <div class="w3-content" style="max-width:700px">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
The Task
</span></h2>
<p>
The task is to generate an utterance from a given MR, which is a) similar to human generated reference texts, and b) highly rated by humans. Similarity will be assessed using standard metrics, such as BLEU and METEOR. Human ratings will be obtained using a mixture of crowd-sourcing and expert annotations. We will also test a suite of novel metrics to estimate the quality of a generated utterance.
</p>
<p>
The metrics used for automatic evaluation are available <a href="https://github.com/tuetschek/e2e-metrics">on Github</a>.
</p>
  </div>
</div>

<!-- News

<div class="w3-container" id="news">
  <div class="w3-content" style="max-width:700px">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
News
</span></h2>
<h3>Task Submissions <span class="w3-medium">(Nov 3, 2017)</span></h3>
</div>
</div>
-->

<!-- Register / download data -->

<div class="w3-container" id="data">
  <div class="w3-content" style="max-width:700px">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
Download Data
</span></h2>
<p>
The <strong>full E2E dataset</strong> is now available for download <a href="https://github.com/tuetschek/e2e-dataset/releases/download/v1.0.0/e2e-dataset.zip">here</a>. 
The package includes a description of the data format.
A paper with a detailed description of the dataset appeared on SIGDIAL 2017 and is also available <a href="https://arxiv.org/abs/1706.09254">on arXiv</a>.
</p>
<p>
To cite the dataset, use:
</p>
<pre>
@inproceedings{novikova2017e2e,
  title={The {E2E} Dataset: New Challenges for End-to-End Generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond\v{r}ej and Rieser, Verena},
  booktitle={Proceedings of the 18th Annual Meeting 
             of the Special Interest Group on Discourse and Dialogue},
  address={Saarbr\"ucken, Germany},
  year={2017},
  note={arXiv:1706.09254},
  url={https://arxiv.org/abs/1706.09254},
}
</pre>

<p>
A <strong>package with the outputs</strong> of all participating systems on the test set as well as <strong>raw human ratings</strong> used for the evaluation is now available for download <a href="https://github.com/tuetschek/e2e-eval/releases/download/v1.0.0/e2e-eval.zip">here</a>. The package includes a short description of the data formats.
</p>
<p>
See the <a href="#papers">Proceedings</a> section below for citing the E2E NLG Challenge results.
</p>
  </div>
</div>

<!-- Baseline -->

<div class="w3-container" id="baseline">
  <div class="w3-content" style="max-width:700px">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
Baseline System
</span></h2>
<p>
We used <a href="https://github.com/UFAL-DSG/tgen">TGen</a> (<a href="http://aclweb.org/anthology/P16-2008">Dusek and Jurcicek, 2016</a>) as the baseline system for the challenge. It is a seq2seq model with attention (<a href="https://arxiv.org/abs/1409.0473">Bahdanau et al., 2015</a>) with added beam search and a reranker penalizing outputs that stray away from the input MR. The baseline scores on the development set are as follows:
</p>
<div class="w3-panel w3-leftbar w3-light-grey">
<table style="width: 30%; text-align: left;">
    <tr><th>Metric</th><th>Score</th></tr>
    <tr><td>BLEU</td><td>0.6925</td></tr>
    <tr><td>NIST</td><td>8.4781</td></tr>
    <tr><td>METEOR</td><td>0.4703</td></tr>
    <tr><td>ROUGE-L</td><td>0.7257</td></tr>
    <tr><td>CIDEr</td><td>2.3987</td></tr>
</table>
</div>
<p>
The full baseline system outputs can be downloaded here for both the <a href="https://github.com/tuetschek/e2e-dataset/releases/download/v1.0.0/baseline-devel-output.tsv">development</a> and <a href="https://github.com/tuetschek/e2e-dataset/releases/download/v1.0.0/baseline-test-output.tsv">test</a> sets (one instance per line).
If you want to run the baseline yourself, basic instructions are provided in the <a href="https://github.com/UFAL-DSG/tgen/tree/master/e2e-challenge">TGen Github repository</a>.
</p>
<p>
The scripts used for evaluation are available <a href="https://github.com/tuetschek/e2e-metrics">on Github</a>.
</p>
  </div>
</div>


<!-- The dates -->

<div class="w3-container" id="dates">
  <div class="w3-content" style="max-width:700px">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
Important Dates
</span></h2>
<dl>
<dt> 13 March 2017:</dt><dd> Registration opens</dd>
<dt> 27 March 2017:</dt><dd> Training and development data are released (MRs + references)</dd>
<dt> 27 June 2017:</dt><dd> The baseline system is released.</dd>
<dt> 16 October 2017:</dt><dd> Test data is released (MRs only)</dd>
<dt> 31 October 2017:</dt><dd> Entry submission deadline</dd>
<dt> 15 November 2017:</dt><dd> Evaluation results are released</dd>
<dt> 15 December 2017:</dt><dd> Participants submit a paper describing their systems</dd>
<dt> 1 March 2018:</dt><dd> Final versions of the description papers due</dd>
<dt> 7 November 2018:</dt><dd> Results presented at INLG </dd>
</dl>
  </div>
</div>

<!-- Submission

<div class="w3-container" id="submission">
  <div class="w3-content" style="max-width:700px">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
Submission
</span></h2>

<dl>
<dt>* What does the test set look like?</dt>
<dd>It is very similar to the development set (e.g. no unknown attributes, values or restaurant names) but the combination of attributes is unique (previously unseen). </dd>

<dt>* How do I submit my system?</dt>
<dd>Please send your system outputs to <strong>e2e-nlg-challenge<span class="atcharhere">googlegroups</span>.com</strong> as a *.tsv file. The file must consist of paired MR and system output, each pair on a new line. </dd>

<dt>* How many systems can I submit?</dt>
<dd>You can submit up to 10 entries per team and you must specify a maximum of 1 or 2 primary systems. If you choose to submit more than one primary system, you need to make sure that your systems are sufficiently different in terms of model architecture and justify separate papers/system descriptions (one paper per primary system).</dd>

<dt>* How will you evaluate my system?</dt>
<dd>All submissions will be automatically evaluated using the previously distributed evaluation script. Primary systems will be evaluated using crowdsourcing (following good practises from the well-established Machine Translation Shared Tasks).</dd>

<dt>* How do I submit my paper?</dt>
<dd>Papers will be submitted via email. Please use the <a href="http://naacl2018.org/call_for_paper.html">NAACL 2018 style and format guidelines</a>.
Please note that papers won't be peer-reviewed on this occasion; as such, it is not necessary to anonymise your paper.
Papers should be max. 8 pages of content. </dd>

<dt>* Will my paper be published?</dt>
<dd>At this point, system descriptions will be published on our website, alongside the evaluation results. Please note that submissions to the challenge are considered non-archival and do not count as duplicate submission w.r.t. conference paper submissions.
We will, however, give you the chance to present your work as part of a special session at INLG 2018.</dd>

<dt>* What if my system does badly?</dt>
<dd>In accordance with ethical considerations of NLP shared tasks (<a href="http://www.ethicsinnlp.org/workshop/pdf/EthNLP08.pdf">Parra Escartín et al., 2017</a>), we allow researchers to publish results anonymously if their system performs in the lower 50% of submissions.</dd>

</dl>

  </div>
</div>
-->

<!-- Results -->

<div class="w3-container" id="results">
  <div class="w3-content">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
Evaluation Results
</span></h2>
  <div class="w3-content" style="max-width:700px">

<p>
We are happy to announce that the interest in the E2E NLG shared task has by far outperformed our expectations.
Heriot-Watt University has set out this challenge for the first time this year, and we received a total of 62 submissions by 17 institutions, with about 1/3 of these submissions coming from industry.
In comparison, the well established <a href="http://www.statmt.org/wmt17/index.html">Conference in Machine Translation WMT’17</a> (running since 2006) got 31 institutions submitting to a total of 8 tasks.
</p>
<p>
<a href="https://drive.google.com/open?id=1SHeZkhZESNIgrbcNQtKK2QG0LoqeOjjO&usp=sharing">
  Participants map<br>
  <img src="e2e-map.png" width="700" height="363"></a>
<!--<iframe src="https://www.google.com/maps/d/embed?mid=1SHeZkhZESNIgrbcNQtKK2QG0LoqeOjjO" width="640" height="360"></iframe>-->
</p>
<p>
A brief summary of the E2E NLG Challenge results is now available in our <a href="https://arxiv.org/abs/1810.01170">INLG 2018 paper</a>, a more detailed analysis is in preparation.
</p>

<h3>Automatic Metrics</h3>
<p>
The automatic evaluation results were obtained using the <a href="https://github.com/tuetschek/e2e-metrics">metrics scripts provided with the baseline</a>.
The table is sortable – just click on the metric you want use for sorting. Click again to reverse the sort.
</p>
  </div>
<table class="w3-small w3-table w3-striped w3-border sortable w3-hoverable">
<thead class="w3-black">
<tr><th>Submitter</th><th>Affiliation</th><th>System name</th><th>P?</th><th>BLEU</th><th>NIST</th><th>METEOR</th><th>ROUGE_L</th><th>CIDEr</th></tr>
</thead>
<tbody>
<tr><td>BASELINE</td><td>Heriot-Watt University</td><td>Baseline</td><td>✓</td><td>0.6593</td><td>8.6094</td><td>0.4483</td><td>0.6850</td><td>2.2338</td></tr>
<tr><td>Biao Zhang</td><td>Xiamen University</td><td>bzhang_submit</td><td>✓</td><td>0.6545</td><td>8.1840</td><td>0.4392</td><td>0.7083</td><td>2.1012</td></tr>
<tr><td>Chen Shuang</td><td>Harbin Institute of Technology</td><td>Abstract-beam1</td><td></td><td>0.5854</td><td>5.4691</td><td>0.3977</td><td>0.6747</td><td>1.6391</td></tr>
<tr><td>Chen Shuang</td><td>Harbin Institute of Technology</td><td>Abstract-beam2</td><td></td><td>0.5916</td><td>5.9477</td><td>0.3974</td><td>0.6701</td><td>1.6513</td></tr>
<tr><td>Chen Shuang</td><td>Harbin Institute of Technology</td><td>Abstract-beam3</td><td></td><td>0.6150</td><td>6.8029</td><td>0.4068</td><td>0.6750</td><td>1.7870</td></tr>
<tr><td>Chen Shuang</td><td>Harbin Institute of Technology</td><td>Abstract-greedy</td><td></td><td>0.6635</td><td>8.3977</td><td>0.4312</td><td>0.6909</td><td>2.0788</td></tr>
<tr><td>Chen Shuang</td><td>Harbin Institute of Technology</td><td>NonAbstract-beam2</td><td></td><td>0.5860</td><td>6.1602</td><td>0.3833</td><td>0.6619</td><td>1.6133</td></tr>
<tr><td>Chen Shuang</td><td>Harbin Institute of Technology</td><td>NonAbstract-beam3</td><td></td><td>0.6088</td><td>6.9790</td><td>0.3899</td><td>0.6628</td><td>1.7015</td></tr>
<tr><td>Chen Shuang</td><td>Harbin Institute of Technology</td><td>Primary_NonAbstract-beam1</td><td>✓</td><td>0.5859</td><td>5.4383</td><td>0.3836</td><td>0.6714</td><td>1.5790</td></tr>
<tr><td>ZHAW</td><td>Zurich University of Applied Sciences</td><td>base</td><td></td><td>0.6544</td><td>8.3391</td><td>0.4448</td><td>0.6783</td><td>2.1438</td></tr>
<tr><td>ZHAW</td><td>Zurich University of Applied Sciences</td><td>primary_1</td><td>✓</td><td>0.5864</td><td>8.0212</td><td>0.4322</td><td>0.5998</td><td>1.8173</td></tr>
<tr><td>ZHAW</td><td>Zurich University of Applied Sciences</td><td>primary_2</td><td>✓</td><td>0.6004</td><td>8.1394</td><td>0.4388</td><td>0.6119</td><td>1.9188</td></tr>
<tr><td>FORGe</td><td>Pompeu Fabra University</td><td>E2E_UPF_1</td><td>✓</td><td>0.4207</td><td>6.5139</td><td>0.3685</td><td>0.5437</td><td>1.3106</td></tr>
<tr><td>FORGe</td><td>Pompeu Fabra University</td><td>E2E_UPF_2</td><td></td><td>0.4113</td><td>6.3293</td><td>0.3686</td><td>0.5593</td><td>1.2467</td></tr>
<tr><td>FORGe</td><td>Pompeu Fabra University</td><td>E2E_UPF_3</td><td>✓</td><td>0.4599</td><td>7.1092</td><td>0.3858</td><td>0.5611</td><td>1.5586</td></tr>
<tr><td>Sheffield NLP</td><td>University of Sheffield</td><td>sheffield_primarySystem1_var1</td><td>✓</td><td>0.6015</td><td>8.3075</td><td>0.4405</td><td>0.6778</td><td>2.1775</td></tr>
<tr><td>Sheffield NLP</td><td>University of Sheffield</td><td>sheffield_primarySystem1_var2</td><td></td><td>0.6233</td><td>8.1751</td><td>0.4378</td><td>0.6887</td><td>2.2840</td></tr>
<tr><td>Sheffield NLP</td><td>University of Sheffield</td><td>sheffield_primarySystem1_var3</td><td></td><td>0.5690</td><td>8.0382</td><td>0.4202</td><td>0.6348</td><td>2.0956</td></tr>
<tr><td>Sheffield NLP</td><td>University of Sheffield</td><td>sheffield_primarySystem1_var4</td><td></td><td>0.5799</td><td>7.9163</td><td>0.4310</td><td>0.6670</td><td>2.0691</td></tr>
<tr><td>Sheffield NLP</td><td>University of Sheffield</td><td>sheffield_primarySystem2_var1</td><td>✓</td><td>0.5436</td><td>5.7462</td><td>0.3561</td><td>0.6152</td><td>1.4130</td></tr>
<tr><td>Sheffield NLP</td><td>University of Sheffield</td><td>sheffield_primarySystem2_var2</td><td></td><td>0.5356</td><td>7.8373</td><td>0.3831</td><td>0.5513</td><td>1.5825</td></tr>
<tr><td>HarvardNLP &amp; Henry Elder</td><td>Harvard SEAS &amp; Adapt</td><td>main_1_support_1</td><td></td><td>0.6581</td><td>8.5719</td><td>0.4409</td><td>0.6893</td><td>2.1065</td></tr>
<tr><td>HarvardNLP &amp; Henry Elder</td><td>Harvard SEAS &amp; Adapt</td><td>main_1_support_2</td><td></td><td>0.6618</td><td>8.6025</td><td>0.4571</td><td>0.7038</td><td>2.3371</td></tr>
<tr><td>HarvardNLP &amp; Henry Elder</td><td>Harvard SEAS &amp; Adapt</td><td>main_1_support_3</td><td></td><td>0.6737</td><td>8.6061</td><td>0.4523</td><td>0.7084</td><td>2.3056</td></tr>
<tr><td>HarvardNLP &amp; Henry Elder</td><td>Harvard SEAS &amp; Adapt</td><td>Primary_main_1</td><td>✓</td><td>0.6496</td><td>8.5268</td><td>0.4386</td><td>0.6872</td><td>2.0850</td></tr>
<tr><td>Heng Gong</td><td>Harbin Institute of Technology</td><td>Primary_test_2</td><td>✓</td><td>0.6422</td><td>8.3453</td><td>0.4469</td><td>0.6645</td><td>2.2721</td></tr>
<tr><td>Heng Gong</td><td>Harbin Institute of Technology</td><td>test_1</td><td></td><td>0.6396</td><td>8.3111</td><td>0.4466</td><td>0.6620</td><td>2.2272</td></tr>
<tr><td>Heng Gong</td><td>Harbin Institute of Technology</td><td>test_3</td><td></td><td>0.6395</td><td>8.3127</td><td>0.4457</td><td>0.6628</td><td>2.2442</td></tr>
<tr><td>Heng Gong</td><td>Harbin Institute of Technology</td><td>test_4</td><td></td><td>0.6395</td><td>8.3127</td><td>0.4457</td><td>0.6628</td><td>2.2442</td></tr>
<tr><td>Adapt</td><td>Adapt</td><td>primary_submission-temperature_1.1</td><td>✓</td><td>0.5092</td><td>7.1954</td><td>0.4025</td><td>0.5872</td><td>1.5039</td></tr>
<tr><td>Adapt</td><td>Adapt</td><td>supporting_submission-temperature_0.9</td><td></td><td>0.5573</td><td>7.7013</td><td>0.4154</td><td>0.6130</td><td>1.8110</td></tr>
<tr><td>Adapt</td><td>Adapt</td><td>supporting_submission-temperature_1.0</td><td></td><td>0.5265</td><td>7.3991</td><td>0.4095</td><td>0.5992</td><td>1.6488</td></tr>
<tr><td>&lt;anonymous 1&gt;</td><td>&lt;anonymous 1&gt;</td><td>&lt;anonymous 1 combined&gt;</td><td></td><td>0.2921</td><td>4.7690</td><td>0.2515</td><td>0.4361</td><td>0.6674</td></tr>
<tr><td>&lt;anonymous 1&gt;</td><td>&lt;anonymous 1&gt;</td><td>&lt;anonymous 1 primary&gt;</td><td>✓</td><td>0.4723</td><td>6.1938</td><td>0.3170</td><td>0.5616</td><td>1.2127</td></tr>
<tr><td>Shubham Agarwal</td><td>NLE</td><td>submission_primary</td><td>✓</td><td>0.6534</td><td>8.5300</td><td>0.4435</td><td>0.6829</td><td>2.1539</td></tr>
<tr><td>Shubham Agarwal</td><td>NLE</td><td>submission_second</td><td></td><td>0.6669</td><td>8.5388</td><td>0.4484</td><td>0.6991</td><td>2.2239</td></tr>
<tr><td>Shubham Agarwal</td><td>NLE</td><td>submission_third</td><td></td><td>0.6676</td><td>8.5416</td><td>0.4485</td><td>0.6991</td><td>2.2276</td></tr>
<tr><td>UCSC-Slug2Slug</td><td>UC Santa Cruz</td><td>Slug2Slug</td><td>✓</td><td>0.6619</td><td>8.6130</td><td>0.4454</td><td>0.6772</td><td>2.2615</td></tr>
<tr class="late"><td>UCSC-Slug2Slug</td><td>UC Santa Cruz</td><td>Slug2Slug-alt (late submission)</td><td>✓</td><td>0.6035</td><td>8.3954</td><td>0.4369</td><td>0.5991</td><td>2.1019</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>NonPrimary_1_test_output_model_11_post</td><td></td><td>0.6536</td><td>8.3293</td><td>0.4550</td><td>0.6805</td><td>2.1050</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>NonPrimary_2_test_output_model_13_post</td><td></td><td>0.6562</td><td>8.3942</td><td>0.4571</td><td>0.6876</td><td>2.1706</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>NonPrimary_3_test_output_beam_5_model_11_post</td><td></td><td>0.6805</td><td>8.7777</td><td>0.4462</td><td>0.6928</td><td>2.3195</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>NonPrimary_4_test_output_beam_5_model_13_post</td><td></td><td>0.6742</td><td>8.6590</td><td>0.4499</td><td>0.6983</td><td>2.3018</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>NonPrimary_5_submission_6</td><td></td><td>0.6208</td><td>8.0632</td><td>0.4417</td><td>0.6692</td><td>2.1127</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>NonPrimary_6_submission_4_beam</td><td></td><td>0.6201</td><td>8.0938</td><td>0.4419</td><td>0.6740</td><td>2.1251</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>NonPrimary_7_submission_4</td><td></td><td>0.6182</td><td>8.0616</td><td>0.4417</td><td>0.6729</td><td>2.0783</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>NonPrimary_8_test_train_only</td><td></td><td>0.4111</td><td>6.7541</td><td>0.3970</td><td>0.5435</td><td>1.4096</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>Primary_1_submission_6_beam</td><td>✓</td><td>0.6336</td><td>8.1848</td><td>0.4322</td><td>0.6828</td><td>2.1425</td></tr>
<tr><td>Thomson Reuters NLG</td><td>Thomson Reuters</td><td>Primary_2_test_train_dev</td><td>✓</td><td>0.4202</td><td>6.7686</td><td>0.3968</td><td>0.5481</td><td>1.4389</td></tr>
<tr><td>UCSC-TNT-NLG</td><td>UC Santa Cruz</td><td>System 1/Primary-Sys1</td><td>✓</td><td>0.6561</td><td>8.5105</td><td>0.4517</td><td>0.6839</td><td>2.2183</td></tr>
<tr><td>UCSC-TNT-NLG</td><td>UC Santa Cruz</td><td>System 1/Sys1-Model1</td><td></td><td>0.6476</td><td>8.4301</td><td>0.4508</td><td>0.6795</td><td>2.1233</td></tr>
<tr><td>UCSC-TNT-NLG</td><td>UC Santa Cruz</td><td>System 2/Primary-Sys2</td><td>✓</td><td>0.6502</td><td>8.5211</td><td>0.4396</td><td>0.6853</td><td>2.1670</td></tr>
<tr><td>UCSC-TNT-NLG</td><td>UC Santa Cruz</td><td>System 2/Sys2-Model1</td><td></td><td>0.6606</td><td>8.6223</td><td>0.4439</td><td>0.6772</td><td>2.1997</td></tr>
<tr><td>UCSC-TNT-NLG</td><td>UC Santa Cruz</td><td>System 2/Sys2-Model2</td><td></td><td>0.6563</td><td>8.5482</td><td>0.4482</td><td>0.6835</td><td>2.1953</td></tr>
<tr><td>UCSC-TNT-NLG</td><td>UC Santa Cruz</td><td>System 2/Sys2-Model3</td><td></td><td>0.3681</td><td>6.6004</td><td>0.3846</td><td>0.5259</td><td>1.5205</td></tr>
<tr><td>UIT-DANGNT</td><td>VNU-HCM University of Information Technology</td><td>test_e2e_result_2 final_TSV</td><td>✓</td><td>0.5990</td><td>7.9277</td><td>0.4346</td><td>0.6634</td><td>2.0783</td></tr>
<!-- added -->
<tr><td>UKP-TUDA</td><td>Technische Universität Darmstadt</td><td>test_e2e-Puzikov</td><td>✓</td><td>0.5657</td><td>7.4544</td><td>0.4529</td><td>0.6614</td><td>1.8206</td></tr>
<!-- /added -->
</tbody>
</table>
<span class="w3-small">Note: “P?” denotes primary submissions.</span>
  <div class="w3-content" style="max-width:700px">
<p>
</p>
<h3>Human Evaluation (updated results)</h3>
<p>
The human evaluation was conducted on the 20 primary systems and the baseline using the CrowdFlower platform. We used our newly developed RankME method (<a href="https://arxiv.org/abs/1803.05928">Novikova et al., 2018</a>) to obtain the ratings. Crowd workers were presented with five randomly selected outputs of different systems corresponding to a single meaning representation, and were asked to rank these systems from the best to worst, ties permitted. A single human-authored reference was provided for comparison. We collected separate ranks for <em>quality</em> and <em>naturalness</em>.
</p>
<p>
<em>Quality</em> is defined as an overall quality of the utterance, in terms of its grammatical correctness, fluency, adequacy and other important factors. When collecting quality ratings, system outputs were presented to crowd workers together with the corresponding meaning representation.
</p>
<p>
<em>Naturalness</em> is defined the extent to which the utterance could have been produced by a native speaker. When collecting naturalness ratings, system outputs were presented to crowd workers without the corresponding meaning representation.
</p>
<p>
If used in a real-life NLG system, <em>quality</em> would be considered the primary measure.
</p>
<p>
The final evaluation results were produced using the TrueSkill algorithm (<a href="http://www.aclweb.org/anthology/W14-3301">Sakaguchi et al., 2014</a>). For naturalness, the algorithm performed 1890 pairwise comparisons per each system (37800 comparisons in total), for quality – 1260 comparisons per system (25200 comparisons in total). In results tables, systems are ordered by their inferred system TrueSkill scores, and clustered. Systems within a cluster are considered tied. The system clusters have been created using bootstrap resampling, with a p-level of p ≤ 0.05.
</p>

<h4>Quality</h4>
<table class="w3-small w3-table w3-striped w3-border w3-hoverable">
<thead class="w3-black">
<tr><th>#</th><th>TrueSkill</th><th>Range</th><th>System name</th><th>Submitter</th></tr>
</thead>
<tbody>
<!-- old
<tr class="w3-border-bottom"><td>1</td><td>0.307</td><td>(1-1)</td><td>&lt;anonymous 2&gt;</td><td>&lt;anonymous 2&gt;</td></tr>
<tr><td class="mid-align" rowspan="14">2</td><td>0.228</td><td>(2-4)</td><td>Heng Gong</td><td>Primary_test_2</td></tr>
<tr><td class="notfirst">0.209</td><td>(2-5)</td><td>BASELINE</td><td>Baseline</td></tr>
<tr><td class="notfirst">0.201</td><td>(2-5)</td><td>UIT-DANGNT</td><td>test_e2e_result_2 final_TSV</td></tr>
<tr class="late"><td class="notfirst">0.170</td><td>(3-6)</td><td>&lt;anonymous 2&gt;</td><td>&lt;anonymous 2 alt&gt; (late submission)</td></tr>
<tr><td class="notfirst">0.145</td><td>(4-7)</td><td>&lt;anonymous 4&gt;</td><td>primary_2</td></tr>
<tr><td class="notfirst">0.102</td><td>(6-9)</td><td>UCSC-TNT-NLG</td><td>System 1/Primary-Sys1</td></tr>
<tr><td class="notfirst">0.079</td><td>(7-10)</td><td>UCSC-TNT-NLG</td><td>System 2/Primary-Sys2</td></tr>
<tr><td class="notfirst">0.052</td><td>(8-11)</td><td>&lt;anonymous 4&gt;</td><td>primary_1</td></tr>
<tr><td class="notfirst">0.050</td><td>(8-11)</td><td>Shubham Agarwal</td><td>submission_primary</td></tr>
<tr><td class="notfirst">0.035</td><td>(9-12)</td><td>FORGe</td><td>E2E_UPF_1</td></tr>
<tr><td class="notfirst">-0.003</td><td>(11-13)</td><td>HarvardNLP &amp; Henry Elder</td><td>Primary_main_1</td></tr>
<tr><td class="notfirst">-0.028</td><td>(12-14)</td><td>Sheffield NLP</td><td>sheffield_primarySystem1_var1</td></tr>
<tr><td class="notfirst">-0.060</td><td>(13-15)</td><td>Thomson Reuters NLG</td><td>Primary_2_test_train_dev</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.083</td><td>(13-15)</td><td>FORGe</td><td>E2E_UPF_3</td></tr>
<tr><td class="mid-align" rowspan="3">3</td><td>-0.146</td><td>(16-17)</td><td>Adapt</td><td>primary_submission-temperature_1.1</td></tr>
<tr><td class="notfirst">-0.169</td><td>(16-18)</td><td>Thomson Reuters NLG</td><td>Primary_1_submission_6_beam</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.191</td><td>(16-18)</td><td>Biao Zhang</td><td>bzhang_submit</td></tr>
<tr><td class="mid-align" rowspan="2">4</td><td>-0.427</td><td>(19-20)</td><td>Chen Shuang</td><td>Primary_NonAbstract-beam1</td></tr>
<tr><td class="notfirst">-0.470</td><td>(19-20)</td><td>Sheffield NLP</td><td>sheffield_primarySystem2_var1</td></tr>
     /old -->
<tr class="w3-border-bottom"><td>1</td><td>0.300</td><td>(1.0, 1.0)</td><td>Slug2Slug</td><td>UCSC-Slug2Slug</td></tr>
<tr><td class="mid-align" rowspan="13">2</td><td>0.228</td><td>(2.0, 4.0)</td><td>ukp-tuda</td><td>UKP-TUDA</td></tr>
<tr><td class="notfirst">0.213</td><td>(2.0, 5.0)</td><td>Primary_test_2</td><td>Heng Gong</td></tr>
<tr><td class="notfirst">0.184</td><td>(3.0, 5.0)</td><td>test_e2e_result_2_final_TSV</td><td>UIT-DANGNT</td></tr>
<tr><td class="notfirst">0.184</td><td>(3.0, 6.0)</td><td>Baseline</td><td>BASELINE</td></tr>
<tr><td class="notfirst">0.136</td><td>(5.0, 7.0)</td><td>Slug2Slug-alt (late submission)</td><td>UCSC-Slug2Slug</td></tr>
<tr><td class="notfirst">0.117</td><td>(6.0, 8.0)</td><td>primary_2</td><td>ZHAW</td></tr>
<tr><td class="notfirst">0.084</td><td>(7.0, 10.0)</td><td>System 1/Primary-Sys1</td><td>UCSC-TNT-NLG</td></tr>
<tr><td class="notfirst">0.065</td><td>(8.0, 10.0)</td><td>System 2/Primary-Sys2</td><td>UCSC-TNT-NLG</td></tr>
<tr><td class="notfirst">0.048</td><td>(8.0, 12.0)</td><td>submission_primary</td><td>NLE</td></tr>
<tr><td class="notfirst">0.018</td><td>(10.0, 13.0)</td><td>primary_1</td><td>ZHAW</td></tr>
<tr><td class="notfirst">0.014</td><td>(10.0, 14.0)</td><td>E2E_UPF_1</td><td>FORGe</td></tr>
<tr><td class="notfirst">-0.012</td><td>(11.0, 14.0)</td><td>sheffield_primarySystem1_var1</td><td>Sheffield NLP</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.012</td><td>(11.0, 14.0)</td><td>Primary_main_1</td><td>HarvardNLP &amp; Henry Elder</td></tr>
<tr><td class="mid-align" rowspan="2">3</td><td>-0.078</td><td>(15.0, 16.0)</td><td>Primary_2_test_train_dev</td><td>Thomson Reuters NLG</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.083</td><td>(15.0, 16.0)</td><td>E2E_UPF_3</td><td>FORGe</td></tr>
<tr><td class="mid-align" rowspan="3">4</td><td>-0.152</td><td>(17.0, 19.0)</td><td>primary_submission-temperature_1.1</td><td>Adapt</td></tr>
<tr><td class="notfirst">-0.185</td><td>(17.0, 19.0)</td><td>Primary_1_submission_6_beam</td><td>Thomson Reuters NLG</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.186</td><td>(17.0, 19.0)</td><td>bzhang_submit</td><td>Biao Zhang</td></tr>
<tr><td class="mid-align" rowspan="2">5</td><td>-0.426</td><td>(20.0, 21.0)</td><td>Primary_NonAbstract-beam1</td><td>Chen Shuang</td></tr>
<tr><td class="notfirst">-0.457</td><td>(20.0, 21.0)</td><td>sheffield_primarySystem2_var1</td><td>Sheffield NLP</td></tr>
</tbody>
</table>

<h4>Naturalness</h4>
<table class="w3-small w3-table w3-striped w3-border w3-hoverable">
<thead class="w3-black">
<tr><th>#</th><th>TrueSkill</th><th>Range</th><th>System name</th><th>Submitter</th></tr>
</thead>
<tbody>
<!-- old
<tr class="w3-border-bottom"><td>1</td><td>0.231</td><td>(1-1)</td><td>Sheffield NLP</td><td>sheffield_primarySystem2_var1</td></tr>
<tr><td class="mid-align" rowspan="10">2</td><td>0.192</td><td>(2-3)</td><td>&lt;anonymous 2&gt;</td><td>&lt;anonymous 2&gt;</td></tr>
<tr><td class="notfirst">0.160</td><td>(2-4)</td><td>Chen Shuang</td><td>Primary_NonAbstract-beam1</td></tr>
<tr><td class="notfirst">0.130</td><td>(4-6)</td><td>HarvardNLP &amp; Henry Elder</td><td>Primary_main_1</td></tr>
<tr><td class="notfirst">0.121</td><td>(4-6)</td><td>Shubham Agarwal</td><td>submission_primary</td></tr>
<tr><td class="notfirst">0.103</td><td>(5-7)</td><td>BASELINE</td><td>Baseline</td></tr>
<tr><td class="notfirst">0.081</td><td>(6-9)</td><td>UIT-DANGNT</td><td>test_e2e_result_2 final_TSV</td></tr>
<tr><td class="notfirst">0.066</td><td>(7-9)</td><td>UCSC-TNT-NLG</td><td>System 2/Primary-Sys2</td></tr>
<tr><td class="notfirst">0.054</td><td>(7-11)</td><td>Heng Gong</td><td>Primary_test_2</td></tr>
<tr><td class="notfirst">0.028</td><td>(9-11)</td><td>UCSC-TNT-NLG</td><td>System 1/Primary-Sys1</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">0.024</td><td>(9-11)</td><td>Biao Zhang</td><td>bzhang_submit</td></tr>
<tr class="late"><td class="mid-align" rowspan="5" style="font-style: normal;">3</td><td>-0.046</td><td>(12-15)</td><td>&lt;anonymous 2&gt;</td><td>&lt;anonymous 2 alt&gt; (late submission)</td></tr>
<tr><td class="notfirst">-0.051</td><td>(12-15)</td><td>Thomson Reuters NLG</td><td>Primary_1_submission_6_beam</td></tr>
<tr><td class="notfirst">-0.078</td><td>(13-16)</td><td>&lt;anonymous 4&gt;</td><td>primary_2</td></tr>
<tr><td class="notfirst">-0.081</td><td>(13-16)</td><td>Sheffield NLP</td><td>sheffield_primarySystem1_var1</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.083</td><td>(13-16)</td><td>&lt;anonymous 4&gt;</td><td>primary_1</td></tr>
<tr><td class="mid-align" rowspan="2">4</td><td>-0.144</td><td>(17-18)</td><td>FORGe</td><td>E2E_UPF_1</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.181</td><td>(17-18)</td><td>Adapt</td><td>primary_submission-temperature_1.1</td></tr>
<tr><td class="mid-align" rowspan="2">5</td><td>-0.262</td><td>(19-20)</td><td>FORGe</td><td>E2E_UPF_3</td></tr>
<tr><td class="notfirst">-0.263</td><td>(19-20)</td><td>Thomson Reuters NLG</td><td>Primary_2_test_train_dev</td></tr>
/old -->
<tr class="w3-border-bottom"><td class="notfirst">1</td><td>0.211</td><td>(1.0, 1.0)</td><td>sheffield_primarySystem2_var1</td><td>Sheffield NLP</td></tr>
<tr><td class="mid-align" rowspan="11">2</td><td>0.171</td><td>(2.0, 3.0)</td><td>Slug2Slug</td><td>UCSC-Slug2Slug</td></tr>
<tr><td class="notfirst">0.154</td><td>(2.0, 4.0)</td><td>Primary_NonAbstract-beam1</td><td>Chen Shuang</td></tr>
<tr><td class="notfirst">0.126</td><td>(3.0, 6.0)</td><td>Primary_main_1</td><td>HarvardNLP &amp; Henry Elder</td></tr>
<tr><td class="notfirst">0.105</td><td>(4.0, 8.0)</td><td>submission_primary</td><td>NLE</td></tr>
<tr><td class="notfirst">0.101</td><td>(4.0, 8.0)</td><td>Baseline</td><td>BASELINE</td></tr>
<tr><td class="notfirst">0.091</td><td>(5.0, 8.0)</td><td>test_e2e_result_2 final_TSV</td><td>UIT-DANGNT</td></tr>
<tr><td class="notfirst">0.077</td><td>(5.0, 10.0)</td><td>ukp-tuda</td><td>UKP-TUDA</td></tr>
<tr><td class="notfirst">0.060</td><td>(7.0, 11.0)</td><td>System 2/Primary-Sys2</td><td>UCSC-TNT-NLG</td></tr>
<tr><td class="notfirst">0.046</td><td>(9.0, 12.0)</td><td>Primary_test_2</td><td>Heng Gong</td></tr>
<tr><td class="notfirst">0.027</td><td>(9.0, 12.0)</td><td>System 1/Primary-Sys1</td><td>UCSC-TNT-NLG</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">0.027</td><td>(10.0, 12.0)</td><td>bzhang_submit</td><td>Biao Zhang</td></tr>
<tr><td class="mid-align" rowspan="5">3</td><td>-0.053</td><td>(13.0, 16.0)</td><td>Primary_1_submission_6_beam</td><td>Thomson Reuters NLG</td></tr>
<tr><td class="notfirst">-0.073</td><td>(13.0, 17.0)</td><td>Slug2Slug-alt (late submission)</td><td>UCSC-Slug2Slug</td></tr>
<tr><td class="notfirst">-0.077</td><td>(13.0, 17.0)</td><td>sheffield_primarySystem1_var1</td><td>Sheffield NLP</td></tr>
<tr><td class="notfirst">-0.083</td><td>(13.0, 17.0)</td><td>primary_2</td><td>ZHAW</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.104</td><td>(15.0, 17.0)</td><td>primary_1</td><td>ZHAW</td></tr>
<tr><td class="mid-align" rowspan="2">4</td><td>-0.144</td><td>(18.0, 19.0)</td><td>E2E_UPF_1</td><td>FORGe</td></tr>
<tr class="w3-border-bottom"><td class="notfirst">-0.164</td><td>(18.0, 19.0)</td><td>primary_submission-temperature_1.1</td><td>Adapt</td></tr>
<tr><td class="mid-align" rowspan="2">5</td><td>-0.243</td><td>(20.0, 21.0)</td><td>Primary_2_test_train_dev</td><td>Thomson Reuters NLG</td></tr>
<tr><td class="notfirst">-0.255</td><td>(20.0, 21.0)</td><td>E2E_UPF_3</td><td>FORGe</td></tr>
</tbody>
</table>

  </div>
  </div>
</div>

<!-- Papers -->

<div class="w3-container" id="papers">
<div class="w3-content">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
Proceedings (Full System Descriptions)
</span></h2>
  <div class="w3-content" style="max-width:700px">
<p>
A brief <a href="https://arxiv.org/abs/1810.01170">description of the challenge results</a> was published at INLG. To cite the challenge, use:
</p>
<pre>
@inproceedings{dusek2018findings,
  title={Findings of the {E2E} {NLG} {Challenge}},
  author={Du{\v{s}}ek, Ond\v{r}ej and Novikova, Jekaterina and Rieser, Verena},
  booktitle={Proceedings of the 11th International Conference 
             on Natural Language Generation},
  address={Tilburg, The Netherlands},
  year={2018},
  note={arXiv:1810.01170},
  url={https://arxiv.org/abs/1810.01170},
}
</pre>
<p>
System outputs and human ratings can now be downloaded from <a href="https://github.com/tuetschek/e2e-eval/releases/download/v1.0.0/e2e-eval.zip">here</a>. Please use the same citation to refer for this data release.
</p>
<p>
All submitters participating in human evaluation provided a description of their primary systems as a technical paper. The papers are linked below:
</p>
<table class="w3-table w3-striped w3-border w3-hoverable w3-small">
<thead class="w3-black">
<tr><td>System</td><td>Paper</td></tr>
</thead>
<tr><td>Adapt</td><td>Henry Elder, Sebastian Gehrmann, Alexander O'Connor and Qun Liu: <a href="final_papers/E2E-Adapt.pdf">E2E NLG Challenge Submission: Towards Controllable Generation of
Diverse Natural Language</a></td></tr>
<tr><td>Chen Shuang</td><td>Suang Chen: <a href="final_papers/E2E-Chen.pdf">A General Model for Neural Text Generation from Structured Data</a></td></tr>
<tr><td>FORGe (both systems)</td><td>Simon Mille and Stamatia Dasiopoulou: <a href="final_papers/E2E-FORGe.pdf">FORGe at E2E 2017</a></td></tr>
<tr><td>HarvardNLP &amp; Henry Elder</td><td>Sebastian Gerhmann, Falcon Z. Dai, Henry Elder and Alexander M. Rush: <a href="final_papers/E2E-HarvardNLP.pdf">End-to-End Content and Plan Selection for Natural Language Generation</a></td></tr>
<tr><td colspan="2">Heng Gong (final paper pending)</td></tr>
<tr><td>NLE</td><td>Shubham Agarwal, Marc Dymetman and Éric Gaussier: <a href="final_papers/E2E-NLE.pdf">A char-based seq2seq submission to the E2E NLG Challenge</a></td></tr>
<tr><td>Sheffield NLP (both systems)</td><td>Mingjie Chen, Gerasimos Lampouras and Andreas Vlachos: <a href="final_papers/E2E-Sheffield.pdf">Sheffield at E2E: structured prediction approaches to end-to-end language generation</a></td></tr>
<tr><td>UCSC-Slug2Slug</td><td>Juraj Juraska, Panagiotis Karagiannis, Kevin K. Bowden and Marilyn A. Walker: <a href="final_papers/E2E-Slug2Slug.pdf">Slug2Slug: A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation</a></td></tr>
<tr><td>UCSC-TNT-NLG, System 1</td><td>Shereen Oraby, Lena Reed, Shubhangi Tandon, Sharath T.S., Stephanie Lukin and Marilyn Walker: <a href="final_papers/E2E-TNT_NLG1.pdf">TNT-NLG, System 1: Using a Statistical NLG to Massively Augment Crowd-Sourced Data for Neural Generation</a></td></tr>
<tr><td>UCSC-TNT-NLG, System 2</td><td>Shubhangi Tandon, Sharath T.S., Shereen Oraby, Lena Reed, Stephanie Lukin and Marilyn Walker: <a href="final_papers/E2E-TNT_NLG2.pdf">TNT-NLG, System 2: Data Repetition and Meaning Representation Manipulation to Improve Neural Generation</a></td></tr>
<tr><td>Thomson Reuters NLP, System 1</td><td>Elnaz Davoodi, Charese Smiley, Dezhao Song and Frank Schilder: <a href="final_papers/E2E-Thomson_Reuters1.pdf">The E2E NLG Challenge: Training a Sequence-to-Sequence Approach for Meaning Representation to Natural Language Sentences</a></td></tr>
<tr><td>Thomson Reuters NLP, System 2</td><td>Charese Smiley, Elnaz Davoodi, Dezhao Song and Frank Schilder: <a href="final_papers/E2E-Thomson_Reuters2.pdf">The E2E NLG Challenge: End-to-End Generation through Partial Template Mining</a></td></tr>
<tr><td>UIT-DANGNT</td><td>Dang Tuan Nguyen and Trung Tran: <a href="final_papers/E2E-UIT_DANGNT.pdf">Structure-based Generation System for E2E NLG Challenge</a></td></tr>
<tr><td>UKP-TUDA</td><td>Yevgeniy Puzikov and Iryna Gurevych: <a href="final_papers/E2E-UKP_TUDA.pdf">E2E NLG Challenge: Neural Models vs. Templates</a></td></tr>
<tr><td>Biao Zhang</td><td>Biao Zhang, Jing Yang, Qian Lin and Jinsong Su: <a href="final_papers/E2E-Zhang.pdf">Attention Regularized Sequence-to-Sequence Learning for E2E NLG Challenge</a></td></tr>
<tr><td>ZHAW (both systems)</td><td>Jan Deriu and Mark Cieliebak: <a href="final_papers/E2E-ZHAW.pdf">End-to-End Trainable System for Enhancing Diversity in Natural Language Generation</a></td></tr>
</table>


<h3 class="w3-margin-top">Other papers using the E2E dataset</h3>
<p>
Published versions of the systems participating in the Challenge:
</p>
<ul class="w3-small">
    <li><a href="http://aclweb.org/anthology/W18-6555">Agarwal et al. (INLG 2018)</a>: <em>Char2char Generation with Reranking for the E2E NLG Challenge</em></li>
    <li><a href="http://aclweb.org/anthology/W18-6556">Elder et al. (INLG 2018)</a>: <em>E2E NLG Challenge Submission: Towards Controllable Generation of Diverse Natural Language</em></li>
    <li><a href="http://aclweb.org/anthology/W18-6557">Puzikov & Gurevych (INLG 2018)</a>: <em>E2E NLG Challenge: Neural Models vs. Templates</em></li>
    <li><a href="http://aclweb.org/anthology/W18-6558">Smiley et al. (INLG 2018)</a>: <em>The E2E NLG Challenge: A Tale of Two Systems</em></li>
    <li><a href="http://aclweb.org/anthology/W18-6503">Deriu & Cieliebak (INLG 2018)</a>: <em>Syntactic Manipulation for Generating more Diverse and Interesting Texts</em></li>
    <li><a href="http://arxiv.org/abs/1810.04700">Gehrmann et al. (INLG 2018)</a>: <em>End-to-End Content and Plan Selection for Data-to-Text Generation</em></li>
    <li><a href="http://arxiv.org/abs/1805.06553">Juraska et al. (NAACL 2018)</a>: <em>A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation</em></li>
</ul>
<p>
Further works that use the E2E dataset but did not participate in the official E2E challenge:
</p>
<ul class="w3-small">
    <li><a href="http://www-labs.iro.umontreal.ca/~lapalme/e2eChallengeDisplay/">Guy Lapalme's realizations using jsRealB</a> -- symbolic realizer, also produces French versions of the outputs. Includes a browser for the outputs, grouping them by slot values.</li>
    <li><a href="http://arxiv.org/abs/1809.07629">Su &amp; Chen (SLT 2018)</a>: <em>Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation</em></li>
    <li><a href="http://arxiv.org/abs/1809.03015">Reed et al. (INLG 2018)</a>: <em>Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?</em></li>
    <li><a href="http://arxiv.org/abs/1809.05288">Juraska &amp; Walker (INLG 2018)</a>: <em>Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs</em></li>
    <li><a href="http://www.aclweb.org/anthology/W18-6543">Shimorina &amp; Gardent (INLG 2018)</a>: <em>Handling Rare Items in Data-to-text Generation</em></li>
    <li><a href="http://arxiv.org/abs/1810.04864">Jagdfeld et al. (INLG 2018)</a>: <em>Sequence-to-Sequence Models for Data-to-Text Natural Language Generation: Word- vs. Character-based Processing and Output Diversity</em></li>
    <li><a href="http://arxiv.org/abs/1808.10122">Wiseman et al. (EMNLP 2018)</a>: <em>Learning Neural Templates for Text Generation</em></li>
    <li><a href="https://arxiv.org/abs/1804.07899">Freitag & Roy (EMNLP 2018)</a>: <em>Unsupervised Natural Language Generation with Denoising Autoencoders</em></li>
    <li><a href="http://arxiv.org/abs/1809.01331">Oraby et al. (Interspeech 2018)</a>: <em>Neural MultiVoice Models for Expressing Novel Personalities in Dialog</em></li>
    <li><a href="https://www.csie.ntu.edu.tw/~yvchen/doc/NAACL18_HNLG.pdf">Su et al. (NAACL 2018)</a>: <em>Natural Language Generation by Hierarchical Decoding with Linguistic Patterns</em></li>
</ul>
</div>
</div>

<!-- Contacts -->

<div class="w3-container" id="contacts" style="padding-bottom: 32px;">
  <div class="w3-content" style="max-width:700px">
<h2 class="w3-center w3-padding-48"><span class="w3-tag w3-padding-left w3-padding-right">
Contacts
</span></h2>

<h4 class="w3-center"><span class="w3-padding-left w3-padding-right">
Organising Comittee
</span></h4>
<p>
Jekaterina Novikova<br/>
Ondrej Dusek<br/>
Verena Rieser
</p>

<p>
Heriot-Watt University, Edinburgh, UK.
</p>

<h4 class="w3-center"><span class="w3-padding-left w3-padding-right">
Contact Details
</span></h4>

<p>
<strong>e2e-nlg-challenge<span class="atcharhere">googlegroups</span>.com</strong>
</p>

<h4 class="w3-center"><span class="w3-padding-left w3-padding-right">
Advisory Committee
</span></h4>

<p>
Mohit Bansal, University of Northern Carolina Chapel Hill <br/>
Ehud Reiter, University of Aberdeen <br/>
Amanda Stent, Bloomberg <br/>
Andreas Vlachos, University of Sheffield <br/>
Marilyn Walker, University of California Santa Cruz <br/>
Matthew Walter, Toyota Technological Institute at Chicago <br/>
Tsung-Hsien Wen, University of Cambridge <br/>
Luke Zettlemoyer, University of Washington<br/>
</p>

  </div>
</div>


<!--
***
End page content
***
-->

</div>

<!-- Footer -->
<footer class="w3-center w3-light-grey w3-padding-48 w3-large">
  <p>Powered by <a href="https://www.w3schools.com/w3css/default.asp" title="W3.CSS" target="_blank" class="w3-hover-text-green">w3.css</a></p>
</footer>


</body>
</html>
